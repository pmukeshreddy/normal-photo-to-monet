{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-22T19:38:52.433115Z","iopub.execute_input":"2023-03-22T19:38:52.433754Z","iopub.status.idle":"2023-03-22T19:39:01.894754Z","shell.execute_reply.started":"2023-03-22T19:38:52.433714Z","shell.execute_reply":"2023-03-22T19:39:01.893625Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"monet_pics_path = tf.io.gfile.glob(\"/kaggle/input/gan-getting-started/monet_tfrec/*.tfrec\")\nphoto_pics_path = tf.io.gfile.glob(\"/kaggle/input/gan-getting-started/photo_tfrec/*.tfrec\")\nimage_shape = [256,256]","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:01.896830Z","iopub.execute_input":"2023-03-22T19:39:01.897639Z","iopub.status.idle":"2023-03-22T19:39:01.918304Z","shell.execute_reply.started":"2023-03-22T19:39:01.897599Z","shell.execute_reply":"2023-03-22T19:39:01.917417Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"monet_pics_path","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:01.919366Z","iopub.execute_input":"2023-03-22T19:39:01.919660Z","iopub.status.idle":"2023-03-22T19:39:01.928712Z","shell.execute_reply.started":"2023-03-22T19:39:01.919634Z","shell.execute_reply":"2023-03-22T19:39:01.927641Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['/kaggle/input/gan-getting-started/monet_tfrec/monet12-60.tfrec',\n '/kaggle/input/gan-getting-started/monet_tfrec/monet16-60.tfrec',\n '/kaggle/input/gan-getting-started/monet_tfrec/monet08-60.tfrec',\n '/kaggle/input/gan-getting-started/monet_tfrec/monet04-60.tfrec',\n '/kaggle/input/gan-getting-started/monet_tfrec/monet00-60.tfrec']"},"metadata":{}}]},{"cell_type":"code","source":"features = {\"image\":tf.io.FixedLenFeature([],tf.string)} # configire the parsing one into fixed lenght\ndef read_tfrecord(images):\n    image_data = tf.io.parse_single_example(images,features) # serlize the data\n    image = image_data[\"image\"]\n    image = tf.image.decode_jpeg(image,channels=3)\n    image = (tf.cast(image,tf.float32)/127.5) - 1\n    image = tf.reshape(image,[*image_shape,3])\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:01.931469Z","iopub.execute_input":"2023-03-22T19:39:01.932262Z","iopub.status.idle":"2023-03-22T19:39:01.939845Z","shell.execute_reply.started":"2023-03-22T19:39:01.932224Z","shell.execute_reply":"2023-03-22T19:39:01.938681Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nmonet_dataset = tf.data.TFRecordDataset(monet_pics_path).map(read_tfrecord, num_parallel_calls=AUTOTUNE).batch(1)\nphoto_dataset = tf.data.TFRecordDataset(photo_pics_path).map(read_tfrecord, num_parallel_calls=AUTOTUNE).batch(1)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:01.941293Z","iopub.execute_input":"2023-03-22T19:39:01.941867Z","iopub.status.idle":"2023-03-22T19:39:04.940426Z","shell.execute_reply.started":"2023-03-22T19:39:01.941832Z","shell.execute_reply":"2023-03-22T19:39:04.939417Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# TO check number of images\nprint( len(list(iter(monet_dataset))))\nprint( len(list(iter(photo_dataset))))","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:04.942113Z","iopub.execute_input":"2023-03-22T19:39:04.942491Z","iopub.status.idle":"2023-03-22T19:39:22.902329Z","shell.execute_reply.started":"2023-03-22T19:39:04.942453Z","shell.execute_reply":"2023-03-22T19:39:22.901213Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"300\n7038\n","output_type":"stream"}]},{"cell_type":"code","source":"something = next(iter(monet_dataset))\nsomething.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:22.904332Z","iopub.execute_input":"2023-03-22T19:39:22.904733Z","iopub.status.idle":"2023-03-22T19:39:22.931357Z","shell.execute_reply.started":"2023-03-22T19:39:22.904695Z","shell.execute_reply":"2023-03-22T19:39:22.930325Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TensorShape([1, 256, 256, 3])"},"metadata":{}}]},{"cell_type":"code","source":"something = next(iter(monet_dataset))\nsomething.shape","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:22.932715Z","iopub.execute_input":"2023-03-22T19:39:22.933050Z","iopub.status.idle":"2023-03-22T19:39:22.958587Z","shell.execute_reply.started":"2023-03-22T19:39:22.933016Z","shell.execute_reply":"2023-03-22T19:39:22.957670Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"TensorShape([1, 256, 256, 3])"},"metadata":{}}]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:22.960352Z","iopub.execute_input":"2023-03-22T19:39:22.960710Z","iopub.status.idle":"2023-03-22T19:39:22.966507Z","shell.execute_reply.started":"2023-03-22T19:39:22.960675Z","shell.execute_reply":"2023-03-22T19:39:22.965080Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"output_channels = 3\nclass cyclic_gans(keras.Model):\n    def __init__(self):\n        super(cyclic_gans,self).__init__()\n        self.monet_genrator = self.make_genrator()\n        self.photo_genrator = self.make_genrator()\n        self.monet_dicriminator = self.make_discriminator()\n        self.photo_dicriminator = self.make_discriminator()\n        self.lambda_cycles = 10\n    def compile(self):\n        super(cyclic_gans,self).compile()  \n        self.monet_genrator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n        self.photo_genrator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n        self.monet_dicriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n        self.photo_dicriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    def create_downsmapler(self,filters,size,apply_instace_norm=True):\n        model = keras.Sequential()\n        model.add(layers.Conv2D(filters,size,strides=2,padding=\"same\",use_bias=False,kernel_initializer = tf.random_normal_initializer(0.0,0.02)))\n        if apply_instace_norm:\n            model.add(tfa.layers.InstanceNormalization(gamma_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02)))\n        model.add(layers.LeakyReLU())\n        return model\n    def create_upsampler(self,filters,size,add_dropout=True):\n        model = keras.Sequential()\n        model.add(layers.Conv2DTranspose(filters,size,strides=2,padding=\"same\",use_bias=True,kernel_initializer = tf.random_normal_initializer(0.0,0.02)))\n        model.add(tfa.layers.InstanceNormalization(gamma_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02)))\n        if add_dropout:\n                  model.add(layers.Dropout(0.5))\n        model.add(layers.LeakyReLU())\n        return model \n    def make_genrator(self):\n        down_samplers = [\n            self.create_downsmapler(64,4,apply_instace_norm=False),\n            self.create_downsmapler(128,4),\n            self.create_downsmapler(256,4),\n            self.create_downsmapler(512,4),\n            self.create_downsmapler(512,4),\n            self.create_downsmapler(512,4),\n            self.create_downsmapler(512,4),\n            self.create_downsmapler(512,4)\n        ]\n        up_samplers = [\n            self.create_upsampler(512,4,add_dropout=True),\n            self.create_upsampler(512,4,add_dropout=True),\n            self.create_upsampler(512,4,add_dropout=True),\n            self.create_upsampler(512,4),\n            self.create_upsampler(256,4),\n            self.create_upsampler(128,4),\n            self.create_upsampler(64,4),\n        ]\n        input_layers = layers.Input(shape=[256,256,3])\n        x =  input_layers\n        skips = []          \n        for downsampler in down_samplers:\n                  x = downsampler(x)\n                  skips.append(x)\n        skips = reversed(skips[:-1])          \n        for upsampler,skip_layer in zip(up_samplers,skips):\n                  x = upsampler(x)\n                  x = layers.Concatenate()([x,skip_layer])\n        last_layer = layers.Conv2DTranspose(output_channels,4,strides=2,padding=\"same\",kernel_initializer = tf.random_normal_initializer(0.0,0.02),activation=\"tanh\")       \n        x = last_layer(x)\n        return keras.Model(inputs=input_layers,outputs=x)\n    def make_discriminator(self):\n        input_layer = layers.Input(shape=[256,256,3],name=\"input_image\")\n        x = input_layer\n        downsampler1 = self.create_downsmapler(64,4,False)(x)\n        downsampler2 = self.create_downsmapler(128,4)(downsampler1)\n        downsampler3 = self.create_downsmapler(256,4)(downsampler2)\n        zero_padd1 = layers.ZeroPadding2D()(downsampler3)\n        conv_layer = layers.Conv2D(512,4,strides=1,use_bias=False,kernel_initializer = tf.random_normal_initializer(0.0,0.02))(zero_padd1)\n        norm_layer1 = tfa.layers.InstanceNormalization(gamma_initializer=keras.initializers.RandomNormal(mean=0.0, stddev=0.02))(conv_layer)\n        leaky_layer = layers.LeakyReLU()(norm_layer1)\n        zero_pad2 = layers.ZeroPadding2D()(leaky_layer)\n        last_layers = layers.Conv2D(1,4,strides=1,kernel_initializer = tf.random_normal_initializer(0.0,0.02))(zero_pad2)\n        return keras.Model(inputs=input_layer,outputs=last_layers)\n    def descriminator_loss_fn(self,real,fake):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True,reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real),real)\n        fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True,reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(fake),fake)\n        return (real_loss+fake_loss)/2\n    def genrator_loss_fn(self,genrated_image):\n        return tf.keras.losses.BinaryCrossentropy(from_logits = True,reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(genrated_image),genrated_image)\n    def cycle_loss_fn(self,image,cycled_image,lambda_image):\n        return tf.reduce_mean(tf.abs(image-cycled_image))*lambda_image\n    def identiy_loss_fn(self,real_photo,photo,lambda_image):\n        return tf.reduce_mean(tf.abs(real_photo-photo))*lambda_image/2\n    def train_step(self,batch_data):\n        real_monet,real_photot = batch_data\n        with tf.GradientTape(persistent=True) as tape:\n            fake_monet = self.monet_genrator(real_photot,training=True)\n            cycled_photo = self.photo_genrator(fake_monet,training=True)\n            fake_photo = self.photo_genrator(real_monet,training=True)\n            cycled_monet = self.monet_genrator(fake_photo,training=True)\n            \n            monet1 = self.monet_genrator(real_monet,training=True)\n            phtot1 = self.photo_genrator(real_photot,training=True)\n            \n            monet_real_discriminated = self.monet_dicriminator(real_monet,training=True)\n            monet_fake_discriminated = self.monet_dicriminator(fake_monet,training=True)\n            photo_real_discriminated = self.photo_dicriminator(real_photot,training=True)\n            photo_fake_discriminated = self.photo_dicriminator(fake_photo,training=True)\n            \n            monet_loss = self.genrator_loss_fn(monet_fake_discriminated)\n            photo_loss = self.genrator_loss_fn(photo_fake_discriminated)\n            \n            cycle_loss = self.cycle_loss_fn(real_monet,cycled_monet,self.lambda_cycles) + self.cycle_loss_fn(real_photot,cycled_photo,self.lambda_cycles)\n            \n            total_monet_genrator_loss = monet_loss + cycle_loss + self.identiy_loss_fn(real_monet,monet1, self.lambda_cycles)\n            total_photo_genrator_loss = photo_loss + cycle_loss + self.identiy_loss_fn(real_photot,phtot1, self.lambda_cycles)\n            \n            monet_discriminator_loss = self.descriminator_loss_fn(monet_real_discriminated,monet_fake_discriminated)\n            photo_discriminator_loss = self.descriminator_loss_fn(photo_real_discriminated,photo_fake_discriminated)\n        monet_genrator_gradient = tape.gradient(total_monet_genrator_loss,self.monet_genrator.trainable_variables)\n        photo_genrator_gradient = tape.gradient(total_photo_genrator_loss,self.photo_genrator.trainable_variables)\n        monet_disr_gradient = tape.gradient(monet_discriminator_loss,self.monet_dicriminator.trainable_variables)\n        photo_disr_gradient = tape.gradient(photo_discriminator_loss, self.photo_dicriminator.trainable_variables)\n        self.monet_genrator_optimizer.apply_gradients(zip(monet_genrator_gradient,self.monet_genrator.trainable_variables))\n        self.photo_dicriminator_optimizer.apply_gradients(zip(photo_disr_gradient,self.photo_dicriminator.trainable_variables))\n        self.photo_genrator_optimizer.apply_gradients(zip(photo_genrator_gradient,self.photo_genrator.trainable_variables))\n        self.monet_dicriminator_optimizer.apply_gradients(zip(monet_disr_gradient,self.monet_dicriminator.trainable_variables))\n        return {\n            \"monet_generator_loss\": total_monet_genrator_loss,\n            \"photo_generator_loss\": total_photo_genrator_loss,\n            \"monet_discriminator_loss\": monet_discriminator_loss,\n            \"photo_discriminator_loss\": photo_discriminator_loss,\n        }","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:22.970402Z","iopub.execute_input":"2023-03-22T19:39:22.970796Z","iopub.status.idle":"2023-03-22T19:39:23.002257Z","shell.execute_reply.started":"2023-03-22T19:39:22.970762Z","shell.execute_reply":"2023-03-22T19:39:23.001083Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cycle_gan = cyclic_gans()\ncycle_gan.compile()","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:23.005161Z","iopub.execute_input":"2023-03-22T19:39:23.005928Z","iopub.status.idle":"2023-03-22T19:39:25.956836Z","shell.execute_reply.started":"2023-03-22T19:39:23.005880Z","shell.execute_reply":"2023-03-22T19:39:25.955787Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"cycle_gan.fit(\n    tf.data.Dataset.zip((monet_dataset.repeat(-1), photo_dataset.repeat(-1))),\n    steps_per_epoch=300,\n    epochs=30\n)","metadata":{"execution":{"iopub.status.busy":"2023-03-22T19:39:25.958138Z","iopub.execute_input":"2023-03-22T19:39:25.958527Z","iopub.status.idle":"2023-03-22T20:19:15.158358Z","shell.execute_reply.started":"2023-03-22T19:39:25.958489Z","shell.execute_reply":"2023-03-22T20:19:15.157106Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch 1/30\n","output_type":"stream"},{"name":"stderr","text":"2023-03-22 19:40:09.403666: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/sequential_8/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"300/300 [==============================] - 145s 258ms/step - monet_generator_loss: 4.6328 - photo_generator_loss: 4.7280 - monet_discriminator_loss: 0.1029 - photo_discriminator_loss: 0.1003\nEpoch 2/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 2.8346 - photo_generator_loss: 2.8913 - monet_discriminator_loss: 0.0022 - photo_discriminator_loss: 0.0020\nEpoch 3/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 2.6176 - photo_generator_loss: 2.6738 - monet_discriminator_loss: 7.1138e-04 - photo_discriminator_loss: 6.5615e-04\nEpoch 4/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 2.4530 - photo_generator_loss: 2.5147 - monet_discriminator_loss: 3.5339e-04 - photo_discriminator_loss: 3.2764e-04\nEpoch 5/30\n300/300 [==============================] - 77s 257ms/step - monet_generator_loss: 2.2618 - photo_generator_loss: 2.3229 - monet_discriminator_loss: 2.0992e-04 - photo_discriminator_loss: 1.9547e-04\nEpoch 6/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 2.0762 - photo_generator_loss: 2.1125 - monet_discriminator_loss: 1.3738e-04 - photo_discriminator_loss: 1.2771e-04\nEpoch 7/30\n300/300 [==============================] - 77s 257ms/step - monet_generator_loss: 2.0349 - photo_generator_loss: 2.0898 - monet_discriminator_loss: 9.5768e-05 - photo_discriminator_loss: 8.9100e-05\nEpoch 8/30\n300/300 [==============================] - 77s 257ms/step - monet_generator_loss: 1.9961 - photo_generator_loss: 2.0641 - monet_discriminator_loss: 6.9364e-05 - photo_discriminator_loss: 6.4695e-05\nEpoch 9/30\n300/300 [==============================] - 77s 257ms/step - monet_generator_loss: 1.9323 - photo_generator_loss: 1.9945 - monet_discriminator_loss: 5.1752e-05 - photo_discriminator_loss: 4.8356e-05\nEpoch 10/30\n300/300 [==============================] - 77s 257ms/step - monet_generator_loss: 1.8761 - photo_generator_loss: 1.9415 - monet_discriminator_loss: 3.9541e-05 - photo_discriminator_loss: 3.6955e-05\nEpoch 11/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.8000 - photo_generator_loss: 1.8603 - monet_discriminator_loss: 3.0709e-05 - photo_discriminator_loss: 2.8724e-05\nEpoch 12/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.7574 - photo_generator_loss: 1.8230 - monet_discriminator_loss: 2.4164e-05 - photo_discriminator_loss: 2.2631e-05\nEpoch 13/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.7266 - photo_generator_loss: 1.7990 - monet_discriminator_loss: 1.9250e-05 - photo_discriminator_loss: 1.8023e-05\nEpoch 14/30\n300/300 [==============================] - 78s 259ms/step - monet_generator_loss: 1.6758 - photo_generator_loss: 1.7434 - monet_discriminator_loss: 1.5514e-05 - photo_discriminator_loss: 1.4519e-05\nEpoch 15/30\n300/300 [==============================] - 78s 259ms/step - monet_generator_loss: 1.6233 - photo_generator_loss: 1.6833 - monet_discriminator_loss: 1.2510e-05 - photo_discriminator_loss: 1.1710e-05\nEpoch 16/30\n300/300 [==============================] - 78s 259ms/step - monet_generator_loss: 1.6018 - photo_generator_loss: 1.6684 - monet_discriminator_loss: 1.0212e-05 - photo_discriminator_loss: 9.5666e-06\nEpoch 17/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.5821 - photo_generator_loss: 1.6491 - monet_discriminator_loss: 8.3428e-06 - photo_discriminator_loss: 7.8164e-06\nEpoch 18/30\n300/300 [==============================] - 77s 257ms/step - monet_generator_loss: 1.5608 - photo_generator_loss: 1.6298 - monet_discriminator_loss: 6.8481e-06 - photo_discriminator_loss: 6.4192e-06\nEpoch 19/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.5255 - photo_generator_loss: 1.5928 - monet_discriminator_loss: 5.6489e-06 - photo_discriminator_loss: 5.2888e-06\nEpoch 20/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.4798 - photo_generator_loss: 1.5405 - monet_discriminator_loss: 4.6733e-06 - photo_discriminator_loss: 4.3696e-06\nEpoch 21/30\n300/300 [==============================] - 78s 259ms/step - monet_generator_loss: 1.4685 - photo_generator_loss: 1.5383 - monet_discriminator_loss: 3.8756e-06 - photo_discriminator_loss: 3.6192e-06\nEpoch 22/30\n300/300 [==============================] - 78s 258ms/step - monet_generator_loss: 1.4318 - photo_generator_loss: 1.4954 - monet_discriminator_loss: 3.2196e-06 - photo_discriminator_loss: 3.0037e-06\nEpoch 23/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.4027 - photo_generator_loss: 1.4664 - monet_discriminator_loss: 2.6692e-06 - photo_discriminator_loss: 2.4943e-06\nEpoch 24/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.3891 - photo_generator_loss: 1.4534 - monet_discriminator_loss: 2.2265e-06 - photo_discriminator_loss: 2.0806e-06\nEpoch 25/30\n300/300 [==============================] - 78s 259ms/step - monet_generator_loss: 1.3705 - photo_generator_loss: 1.4356 - monet_discriminator_loss: 1.8579e-06 - photo_discriminator_loss: 1.7344e-06\nEpoch 26/30\n300/300 [==============================] - 78s 258ms/step - monet_generator_loss: 1.3648 - photo_generator_loss: 1.4337 - monet_discriminator_loss: 1.5530e-06 - photo_discriminator_loss: 1.4481e-06\nEpoch 27/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.3355 - photo_generator_loss: 1.4027 - monet_discriminator_loss: 1.2961e-06 - photo_discriminator_loss: 1.2063e-06\nEpoch 28/30\n300/300 [==============================] - 78s 259ms/step - monet_generator_loss: 1.3029 - photo_generator_loss: 1.3610 - monet_discriminator_loss: 1.0863e-06 - photo_discriminator_loss: 1.0093e-06\nEpoch 29/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.2825 - photo_generator_loss: 1.3375 - monet_discriminator_loss: 9.1008e-07 - photo_discriminator_loss: 8.4361e-07\nEpoch 30/30\n300/300 [==============================] - 77s 258ms/step - monet_generator_loss: 1.2613 - photo_generator_loss: 1.3149 - monet_discriminator_loss: 7.6345e-07 - photo_discriminator_loss: 7.0666e-07\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f775f60c550>"},"metadata":{}}]},{"cell_type":"code","source":"from io import BytesIO\nfrom PIL import Image\nfrom zipfile import ZipFile\nimport numpy as np\n\n\nwith ZipFile(\"images.zip\",mode=\"w\") as zipfiles:\n    i = 1\n    for img in photo_dataset:\n        genrated_image_cycle = cycle_gan.monet_genrator(img,training=False)[0].numpy()\n        scaled_img = (genrated_image_cycle*127.5+127.5).astype(np.uint8)\n        with BytesIO() as image_bytes:\n            Image.fromarray(scaled_img).save(image_bytes, 'JPEG')\n            image_bytes.seek(0)\n            zipfiles.writestr('{}.jpg'.format(i), image_bytes.read())\n            i += 1","metadata":{"execution":{"iopub.status.busy":"2023-03-22T20:26:43.837643Z","iopub.execute_input":"2023-03-22T20:26:43.838230Z","iopub.status.idle":"2023-03-22T20:35:10.889150Z","shell.execute_reply.started":"2023-03-22T20:26:43.838193Z","shell.execute_reply":"2023-03-22T20:35:10.888124Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}